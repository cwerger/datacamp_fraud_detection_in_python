{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(df):\n",
    "    X = df.iloc[:, 1:29]\n",
    "    X = np.array(X).astype(np.float)\n",
    "    y = df.iloc[:, 29]\n",
    "    y=np.array(y).astype(np.float)\n",
    "    return X,y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X,y):\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Class #0\", alpha=0.5, linewidth=0.15)\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Class #1\", alpha=0.5, linewidth=0.15, c='y')\n",
    "    plt.legend()\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_plot(X,y,X_resampled,y_resampled, method):\n",
    "    # Start a plot figure\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    # sub-plot number 1, this is our normal data\n",
    "    c0 = ax1.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Class #0\",alpha=0.5)\n",
    "    c1 = ax1.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Class #1\",alpha=0.5, c='r')\n",
    "    ax1.set_title('Original set')\n",
    "    # sub-plot number 2, this is our oversampled data\n",
    "    ax2.scatter(X_resampled[y_resampled == 0, 0], X_resampled[y_resampled == 0, 1], label=\"Class #0\", alpha=.5)\n",
    "    ax2.scatter(X_resampled[y_resampled == 1, 0], X_resampled[y_resampled == 1, 1], label=\"Class #1\", alpha=.5,c='r')\n",
    "    ax2.set_title(method)\n",
    "    # some settings and ready to go\n",
    "    plt.figlegend((c0, c1), ('Class #0', 'Class #1'), loc='lower center',\n",
    "                  ncol=2, labelspacing=0.)\n",
    "    #plt.tight_layout(pad=3)\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(recall, precision, average_precision):\n",
    "    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results(X_train, y_train, X_test, y_test, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    predicted = model.predict(X_test)\n",
    "    probs = model.predict_proba(X_test)\n",
    "    print (classification_report(y_test, predicted))\n",
    "    print (confusion_matrix(y_test, predicted))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(km_cm):\n",
    "    df_cm = pd.DataFrame(km_cm, ['True Normal','True Fraud'],['Pred Normal','Pred Fraud'])\n",
    "    plt.figure(figsize = (8,4))\n",
    "    sn.set(font_scale=1.4) \n",
    "    sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16},fmt='g')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_details(ldamodel, corpus):\n",
    "    topic_details_df = pd.DataFrame()\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                topic_details_df = topic_details_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "    topic_details_df.columns = ['Dominant_Topic', '% Score', 'Topic_Keywords']\n",
    "    return topic_details_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda]",
   "language": "python",
   "name": "conda-env-Anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
